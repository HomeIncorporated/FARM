{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models with AWS SageMaker\n",
    "\n",
    "SageMaker is a managed service to train and deploy ML models. This tutorial is a quickstart guide to train models on SageMaker with FARM. It walks through on how to train a Roberta Model for Question Answering task.\n",
    "\n",
    "### Prerequisites\n",
    "* AWS IAM Role with access to SageMaker and S3.\n",
    "* A directory containing the training script(more details in the next section) that you want to run and a requirements.txt file for the dependencies(including FARM). You can find examples at https://github.com/deepset-ai/FARM/tree/master/examples.\n",
    "* (optional) Cleaned/processed training data uploaded to a S3 bucket. If no train/dev data paths are supplied, the sample public datasets provided by FARM are used. \n",
    "\n",
    "### Train Script\n",
    "Any of the FARM examples can be used for training with SageMaker with slight adjustments. The config/hyperparameters are written by SageMaker at `/opt/ml/input/config/hyperparameters.json`. The train script can read the file and set the parameters accordingly.\n",
    "\n",
    "For this tutorial, there is a directory `train_with_sagemaker` at the same dir level of this notebook. It contains `question_answering.py` and a `requirements.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.estimator import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"arn:aws:iam::xxxxxxxxxxxx:role/service-role/AmazonSageMaker-ExecutionRole-20191204Txxxxxx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Hyperparameters are passed to the Estimator(next step) and they are made available to the train script during training as a JSON file located at `/opt/ml/input/config/hyperparameters.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"batch_size\": 24,\n",
    "    \"n_epochs\": 2,\n",
    "    \"evaluate_every\": 200,\n",
    "    \"save_dir\": \"/opt/ml/model\",\n",
    "    \n",
    "    # If no train/dev data paths are supplied, the sample public datasets provided by FARM are used for training\n",
    "    # \"train_filename\": \"<file-name>\",\n",
    "    # \"dev_filename\": \"<file-name>\",\n",
    "    # \"data_dir\": \"/opt/ml/input/data/train/\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator\n",
    "\n",
    "`Estimator` is a high level abstraction for handling the training task on SageMaker. `PyTorch` Estimator builds a container with a specific version of PyTorch, install dependencies as supplied in the `source_dir`, and executes the training script as specified by `entry_point`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    base_job_name=\"roberta_qa_train\",\n",
    "    entry_point=\"question_answering.py\",\n",
    "    source_dir=\"train_with_sagemaker\",  # the dir contains the train script and a requirements.txt file\n",
    "    framework_version=\"1.3.1\", # PyTorch version\n",
    "    train_instance_count=1,\n",
    "    role=role,  # IAM role to assume for execution\n",
    "    hyperparameters=hyperparameters,\n",
    "    \n",
    "    # For testing locally first, set instance type as \"local\". To run on the cloud, set this \n",
    "    # to an EC2 instance type, for example, \"ml.p3.8xlarge\"\n",
    "    train_instance_type=\"local\", \n",
    "    # output_path=\"s3://path-to-folder\", # the S3 path where model is outputted \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training\n",
    "\n",
    "The `fit()` method starts a training job. It takes the S3 path of the train data as an argument. `wait` argument specifies whether the call should wait for the training job to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(\n",
    "    # Set the S3 bucket for train data \n",
    "    # input={\"train\": \"s3://<path-to-train-data>\"}, \n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the training job on the SageMaker Console\n",
    "\n",
    "You can view the newly created training job in the SageMaker console in the \"Training jobs\" view, under the \"Training\" section. You can view the log stream and the resource utilization metrics in the console. (screenshots\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Spot Instances (optional)\n",
    "SageMaker provides managed Spot Instances that provides significant cost savings as compared to on-demand instances. The `Trainer` in FARM has built-in checkpointing functionality that allows saves periodic checkpoint(state of the dataloaders, models, optimizers, and schedulers), so after an interruption, the training can be resumed from the previous state."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
